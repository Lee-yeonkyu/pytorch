{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"images\")\n",
    "    os.mkdir(\"images/dogs\")\n",
    "    os.mkdir(\"images/cats\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jong\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 1\n",
    "learning_rate = 0.0002\n",
    "num_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨(혹은 클래스) 별로 폴더가 저장되어 있는 루트 디렉토리를 지정합니다.\n",
    "img_dir = \"./images\"\n",
    "\n",
    "# 해당 루트 디렉토리를 ImageFolder 함수에 전달합니다.\n",
    "# 이때 이미지들에 대한 변형도 같이 전달해줍니다.\n",
    "img_data = dset.ImageFolder(\n",
    "    img_dir,\n",
    "    transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),  # 이미지 크기를 256x256으로 바꿔줍니다.\n",
    "            transforms.RandomResizedCrop(\n",
    "                224\n",
    "            ),  # 256x256 이미지의 랜덤한 위치에서 224x224 크기만큼 샘플링 합니다.\n",
    "            transforms.RandomHorizontalFlip(),  # 랜덤한 확률로 이미지를 좌우반전 합니다.\n",
    "            transforms.ToTensor(),  # 이미지 데이터를 텐서로 변형합니다.\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "    img_data, batch_size=batch_size, shuffle=True, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컨볼루션 연산이 2번 연속하는 경우\n",
    "# 컨볼루션-활성화함수-컨볼루션-활성화함수-풀링\n",
    "def conv_2_block(in_dim, out_dim):\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(in_dim, out_dim, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_dim, out_dim, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# 컨볼루션 연산이 3번 연속하는 경우\n",
    "# 컨볼루션-활성화함수-컨볼루션-활성화함수-컨볼루션-활성화함수-풀링\n",
    "def conv_3_block(in_dim, out_dim):\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(in_dim, out_dim, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_dim, out_dim, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_dim, out_dim, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, base_dim, num_classes=2):\n",
    "        super(VGG, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            conv_2_block(3, base_dim),\n",
    "            conv_2_block(base_dim, 2 * base_dim),\n",
    "            conv_3_block(2 * base_dim, 4 * base_dim),\n",
    "            conv_3_block(4 * base_dim, 8 * base_dim),\n",
    "            conv_3_block(8 * base_dim, 8 * base_dim),\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(8 * base_dim * 7 * 7, 100),\n",
    "            nn.ReLU(\n",
    "                True\n",
    "            ),  # True 는 inplace 연산을 하겠다는 의미를 가집니다. inplace 연산은 결과값을 새로운 변수에 값을 저장하는 대신 기존의 데이터를 대체하는것을 의미합니다.\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(100, 20),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(20, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        x = x.view(x.size(0), -1)  # x.size(0)를 batch size로 바꿔도 같은 값입니다.\n",
    "        x = self.fc_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "('feature', Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "))\n",
      "('fc_layer', Sequential(\n",
      "  (0): Linear(in_features=6272, out_features=100, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Linear(in_features=100, out_features=20, bias=True)\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): Linear(in_features=20, out_features=2, bias=True)\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "# gpu가 사용 가능한 경우에는 device를 0번 gpu로 설정하고 불가능하면 cpu로 설정합니다.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# 앞서 정의한대로 vGG 클래스를 인스턴스화 하고 지정한 장치에 올립니다.\n",
    "model = VGG(base_dim=16).to(device)\n",
    "\n",
    "# 손실함수 및 최적화함수를 설정합니다.\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 모델 자녀 노드의 이름과 모듈을 출력합니다.\n",
    "for i in model.named_children():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7507, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7440, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6555, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7308, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7148, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6780, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6794, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6825, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epoch):\n",
    "    for j, [image, label] in enumerate(train_loader):\n",
    "        x = image.to(device)\n",
    "        y_ = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)\n",
    "        loss = loss_func(output, y_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
