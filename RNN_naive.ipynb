{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\jong\\anaconda3\\envs\\pytorch\\lib\\site-packages (1.3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "\n",
    "# chunk에 대한 설명은 아래 함수정의하면서 하겠습니다.\n",
    "chunk_len = 200\n",
    "\n",
    "hidden_size = 100\n",
    "batch_size = 1\n",
    "num_layers = 1\n",
    "embedding_size = 70\n",
    "lr = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\u000b\f\n",
      "num_chars =  100\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "\n",
    "n_characters = len(all_characters)\n",
    "print(all_characters)\n",
    "print(\"num_chars = \", n_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 1115394\n"
     ]
    }
   ],
   "source": [
    "file = unidecode.unidecode(open(\"./data/input.txt\").read())\n",
    "file_len = len(file)\n",
    "print(\"file_len =\", file_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce,--\n",
      "\n",
      "MENENIUS:\n",
      "Well, well, no more of that.\n",
      "\n",
      "CORIOLANUS:\n",
      "Though there the people had more absolute power,\n",
      "I say, they nourish'd disobedience, fed\n",
      "The ruin of the state.\n",
      "\n",
      "BRUTUS:\n",
      "Why, shall the people\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 파일의 일부분을 랜덤하게 불러오는 코드\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len +1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "print(random_chunk())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36, 37, 38, 13, 14, 15])\n"
     ]
    }
   ],
   "source": [
    "# 문자열을 받았을때 이를 인덱스의 배열로 바꿔주는 함수\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor\n",
    "\n",
    "print(char_tensor('ABCdef'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤한 텍스트 chunk를 불러와서 이를 입력과 목표값을 바꿔주는 함수\n",
    "# pytorch라는 문자열이 들어오면 입력은 pytorc / 목표값은 ytorch가 된다.\n",
    "def random_training_set():\n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, embedding_size, hidden_size, output_size, num_layers=1\n",
    "    ):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.encoder = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        self.rnn = nn.RNN(self.embedding_size, self.hidden_size, self.num_layers)\n",
    "        self.decoder = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        out = self.encoder(input.view(1, -1))\n",
    "        out, hidden = self.rnn(out, hidden)\n",
    "        out = self.decoder(out.view(batch_size, -1))\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden\n",
    "\n",
    "\n",
    "model = RNN(n_characters, embedding_size, hidden_size, n_characters, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(\n",
    "    input_size=n_characters,\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=n_characters,\n",
    "    num_layers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36])\n",
      "torch.Size([2, 1, 100])\n",
      "torch.Size([1, 100])\n"
     ]
    }
   ],
   "source": [
    "inp = char_tensor(\"A\")\n",
    "print(inp)\n",
    "hidden = model.init_hidden()\n",
    "print(hidden.size())\n",
    "out, hidden = model(inp, hidden)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    start_str = \"b\"\n",
    "    inp = char_tensor(start_str)\n",
    "    hidden = model.init_hidden()\n",
    "    x = inp\n",
    "    \n",
    "    print(start_str,end=\"\")\n",
    "    for i in range(200):\n",
    "        output, hidden = model(x, hidden)\n",
    "        output_dist = output.data.view(-1).div(0.8).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        predicted_char = all_characters[top_i]\n",
    "        \n",
    "        print(predicted_char, end=\"\")\n",
    "        \n",
    "        x = char_tensor(predicted_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tensor([2.3685], grad_fn=<DivBackward0>) \n",
      "\n",
      "breare, he the yimer h thas procer dor hatis foue on bich thee will darens thao non weay, frosheek ingr it our cavend pther as sor\n",
      "What, th sour ptay theats? Kof doung dowe are moy thertret mears note \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1511], grad_fn=<DivBackward0>) \n",
      "\n",
      "be not mence come whow the lered pey you that ou eyou, mortse diend nut nom you dand he than' nom doun noce uen the ward fies're, beron the hy nowk\n",
      ":\n",
      "Aigt mo.\n",
      "Wo Woke aw and too to Ipen stanm hend, to \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1581], grad_fn=<DivBackward0>) \n",
      "\n",
      "besies.\n",
      "\n",
      "Rrilist of will beather this mome nien momruvonge how feale our it ald hou, the warders in wremarrentem, heat Courden earounst our,\n",
      "And me pepit out thes ghis me thy of hach'st loum cawerdest \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1161], grad_fn=<DivBackward0>) \n",
      "\n",
      "breoters.\n",
      "\n",
      "KENHIO:\n",
      "I hattom live,\n",
      "And shend ast be sorred is the hit ferle of my hen me sies ast thou my noke to lome wither of that bpis heare and loves ke thee but pilp be that momane noss:\n",
      "-houe com\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8846], grad_fn=<DivBackward0>) \n",
      "\n",
      "bewinds\n",
      "That to hister:\n",
      "That Fagved lialwes more and mest be gucd, I beorn houe as as thears malles, I tey myour hety da me siof fingeray forjrous.\n",
      "\n",
      "SLARTIO:\n",
      "To of sore this divess your land have would\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.7652], grad_fn=<DivBackward0>) \n",
      "\n",
      "bul hime son in a see?\n",
      "I gould priit foring the make the cowreaverreat afmar I to dare here\n",
      "In the hand till ble the lave trostere harther was thou as of she goire, to might she raunce my that a choust\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9431], grad_fn=<DivBackward0>) \n",
      "\n",
      "ble qulis, comure, me him he me so do? and brainion.\n",
      "\n",
      "PIONTELO:\n",
      "Be, have suld sho with and gomo fight hess, trand gut in hear's sin and he lekes me will speark the havilling surdint so hapk on,\n",
      "Will as\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9100], grad_fn=<DivBackward0>) \n",
      "\n",
      "beth bport.\n",
      "\n",
      "GENVER:\n",
      "But corse, shathing hear.\n",
      "\n",
      "KING RIOHO:\n",
      "Ungur noth or with you shere you breatht your a buth.\n",
      "\n",
      "LAMEOR III:\n",
      "Seair some\n",
      "Thingurst\n",
      "What nob as ade and some till bring to martes\n",
      "So thy \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.7611], grad_fn=<DivBackward0>) \n",
      "\n",
      "by his no stise wond of the wind the souk firsent bartion, in play us come of seave will that you the was borning of botes.\n",
      "\n",
      "RUSET:\n",
      "And so be the for the wast praid here on in Bigher,\n",
      "Ond whine eak\n",
      "Fou\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1668], grad_fn=<DivBackward0>) \n",
      "\n",
      "blesty be to gaturgen and I more hall.\n",
      "\n",
      "Hoth masherercass, the hanss air'd our morless agrow thand Rome to ades the qudesforisurder, and womess chanded me the eirst whighelm say that carm.\n",
      "What behy hi\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0828], grad_fn=<DivBackward0>) \n",
      "\n",
      "bue slears us worth the towerce us peerent, to sires are a biinterring death the my is my you his best sear! show liesinese beereng to sarake?\n",
      "\n",
      "DICHARIS:\n",
      "To speature, ese to monce, not that them,\n",
      "The p\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8770], grad_fn=<DivBackward0>) \n",
      "\n",
      "bly monion the sarm or the wance so.\n",
      "\n",
      "Horm and sir,\n",
      "The tamm, and must,\n",
      "I grows, to the I seath, thou and and\n",
      "In my crive, not seath she havino, theal in wearsing hardat, peruyster my sensing that Guan\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.7364], grad_fn=<DivBackward0>) \n",
      "\n",
      "ble suld to not with on then her agoq\n",
      "Were darchect of tare\n",
      "Tur youry highing now my hard prome for the some here here on, hope on there, and where of be twither,\n",
      "Of the Pide sun unsend tom not her mor\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8629], grad_fn=<DivBackward0>) \n",
      "\n",
      "blle was to gruck the know grown:\n",
      "To do good, one for thank saijtious rey to that sey,\n",
      "So the all to your serse and as to sund shall has that downded and werved but non.\n",
      "\n",
      "Second reasies be the wows of \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8334], grad_fn=<DivBackward0>) \n",
      "\n",
      "be word. Or would deinfings,\n",
      "To huse.\n",
      "\n",
      "FOr ELIUS:\n",
      "Beatiot see thou to day are me\n",
      "in marrato!\n",
      "\n",
      "CORIO:\n",
      "That it, you, be that I come tuman.\n",
      "\n",
      "KESTENTIO:\n",
      "The bet from muchiobly, one's could as mour of you t\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9150], grad_fn=<DivBackward0>) \n",
      "\n",
      "beece hole, the with soneed what s\n",
      "Glanguo should loender'd and conour sucder to slear comes and partiturioat it in and, should legs.\n",
      "\n",
      "CLIUTINV:\n",
      "Beath'd his be resor,\n",
      "Sear your this ladding hither's\n",
      "au\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8413], grad_fn=<DivBackward0>) \n",
      "\n",
      "bugy all for for now, go child the durdest:\n",
      "Then my beter to will sover hell ponself.\n",
      "\n",
      "GLORe are can is and will.\n",
      "He a respeed the did, my lord, no my dooty;\n",
      "The doad to more for arder ryed what will t\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.7680], grad_fn=<DivBackward0>) \n",
      "\n",
      "begure is neath years gears the seate not bick in I so first not sweet he viked me preashers leinine:\n",
      "If the love by a the can decerteming that soop up,\n",
      "I condiace, be be they this muse's of his laster\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9976], grad_fn=<DivBackward0>) \n",
      "\n",
      "blo comes most chare way our hye,\n",
      "Which? Wance\n",
      "And sell wine prament, so king my launter that our the be sir, I come say our styen my groughter,\n",
      "So muchire siles: he way subed; are comey's carses,\n",
      "Nor \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8188], grad_fn=<DivBackward0>) \n",
      "\n",
      "bmblity-kerio, hould slaaks\n",
      "And no more ope\n",
      "Let freenon:\n",
      "Go.\n",
      "\n",
      "CELANDIO:\n",
      "My essear's more fain.\n",
      "\n",
      "MENTINGLENENIUS:\n",
      "As starie is? Come mooghtion at hander, thou hear, be the sin's made of a day his alast?\n",
      " ====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    inp, label = random_training_set()\n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    loss = torch.tensor([0]).type(torch.FloatTensor)\n",
    "    optimizer.zero_grad()\n",
    "    for j in range(chunk_len-1):\n",
    "        x=inp[j]\n",
    "        y_ = label[j].unsqueeze(0).type(torch.LongTensor)\n",
    "        y, hidden = model(x,hidden)\n",
    "        loss += loss_func(y,y_)\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"\\n\",loss/chunk_len,\"\\n\")\n",
    "        test()\n",
    "        print(\"\\n\",\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
